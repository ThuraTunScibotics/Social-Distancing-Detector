{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed3f46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from scipy.spatial import distance as dist\n",
    "import numpy as np\n",
    "import imutils\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d70201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/thura/Desktop/TSF-internship/social-distancing-detector/notebook-file'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5424a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration \n",
    "\n",
    "# base path to YOLO directory\n",
    "MODEL_PATH = '/home/thura/Desktop/TSF-internship/social-distancing-detector/yolo-coco/'\n",
    "\n",
    "# initialize minimum probability to filter weak detections along-with\n",
    "# the threshold when applying non-maxima supression\n",
    "MIN_CONF = 0.3\n",
    "NMS_THRESH = 0.3\n",
    "\n",
    "# boolean indicating if NIVIDIA CUDA should be used\n",
    "USE_GPU = True\n",
    "\n",
    "# define the minimum safe distance (in pixels) that two people can be\n",
    "# frome each other\n",
    "MIN_DISTANCE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7d92c",
   "metadata": {},
   "source": [
    "### Detection People Using YOLO\n",
    "Forward pass the YOLO object detector to the input frame, and then calculate and implement coding for getting return results of bouding boxes, confidences scores and centroids of each object with class people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f486331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people(frame, yolo, ln, personIdx=0):\n",
    "    \n",
    "    # grab the dimensions of the frame and initialize the list of the results\n",
    "    (H, W) = frame.shape[:2]\n",
    "    results = []\n",
    "\n",
    "    # construct a blob from the input frame and then \n",
    "    # forward pass YOLO object detector, give bounding boxes on people\n",
    "    # and associated probabilities\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416),\n",
    "                               swapRB = True, crop = False)\n",
    "\n",
    "    yolo.setInput(blob)\n",
    "    layerOutputs = yolo.forward(ln)\n",
    "\n",
    "    # initialize our lists of detected bounding boxes, centroid and\n",
    "    # confidences, respectively\n",
    "    boxes = []\n",
    "    centroids = []\n",
    "    confidences = []\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            # extract the class ID and confidence (i.e, probability)\n",
    "            # of the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # filter detection by (1) ensuring that the object detected was a person\n",
    "            # and (2) that the minimum confidence is met\n",
    "            if classID == personIdx and confidence > MIN_CONF:\n",
    "                # scale the bounding box coordinates back relative to\n",
    "                # the size of the image, keeping in mind that YOLO \n",
    "                # actually returns the certer (x, y)- coordinate of\n",
    "                # the bounding box followed by the boxes' width & height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype('int')\n",
    "\n",
    "                # use the center (x, y) coordinates to derive the top\n",
    "                # and the left corner of the bounding box\n",
    "                x = int(centerX - (width/2))\n",
    "                y = int(centerY - (height/2))\n",
    "\n",
    "                # update our list of bounding box coordinates,\n",
    "                # centroids, and confidences\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                centroids.append((centerX, centerY))\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    # apply non-maxima suppression to suppress weak, overlapping\n",
    "    # bounding boxes\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
    "\n",
    "    # ensure at least one detection exists\n",
    "    if len(idxs) > 0:\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in idxs.flatten():\n",
    "            # extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "            # update our results list to consist of the person\n",
    "            # predicrion probability, bounding box coordinates,\n",
    "            # and the centroid\n",
    "            r = (confidences[i], (x, y, x+w, y+h), centroids[i])\n",
    "            results.append(r)\n",
    "\n",
    "    # return the list of results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57c8964b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO from disk...\n",
      "[NIFO] setting preferable backend to OpenCV and target to CUDA...\n"
     ]
    }
   ],
   "source": [
    "## Require parameters for detect_people function\n",
    "# load the COCO class labels our YOLO model was trained on\n",
    "labelsPath = os.path.join(MODEL_PATH, \"coco.names\")\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "# derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = os.path.join(MODEL_PATH, \"yolov3.weights\")\n",
    "configPath = os.path.join(MODEL_PATH, \"yolov3.cfg\")\n",
    "\n",
    "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "yolo = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "\n",
    "# Check if we are going to use GPU\n",
    "if USE_GPU:\n",
    "    # set CUDA as the preferable backend to OpenCV and target to OPENCL_FP16\n",
    "    print(\"[NIFO] setting preferable backend to OpenCV and target to CUDA...\")\n",
    "    yolo.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "    yolo.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n",
    "\n",
    "# determine only the output layer names that we need from YOLO\n",
    "layer_names = yolo.getLayerNames()\n",
    "layer_names = [layer_names[i - 1] for i in yolo.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14f165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INFO accessing video stream....']\n"
     ]
    }
   ],
   "source": [
    "# Getting Input Video File\n",
    "\n",
    "input_data = '/home/thura/Desktop/TSF-internship/social-distancing-detector/pedestrians.mp4'\n",
    "output_data = '/home/thura/Desktop/TSF-internship/social-distancing-detector/output.avi'\n",
    "\n",
    "print([\"INFO accessing video stream....\"])\n",
    "vs = cv2.VideoCapture(input_data)\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6de13eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    # read the next frame from the file\n",
    "    ret, frame = vs.read()\n",
    "\n",
    "    # if the frame was not return, then we have reached the end of the stream\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # resize the frame and then detect people (and only people) in it\n",
    "    # using function above\n",
    "    frame = imutils.resize(frame, width=700)\n",
    "\n",
    "    results = detect_people(frame, yolo, layer_names, personIdx=LABELS.index(\"person\"))\n",
    "\n",
    "    # initialize the set of indexes that violate the minimum social distance\n",
    "    violate = set()\n",
    "\n",
    "    # ensure there are *at least* two people detection (required in\n",
    "    # order to compute our pairwise distance maps)\n",
    "    if len(results) >= 2:\n",
    "        # extract all centroids from the results and compute the \n",
    "        # Euclidean distances between all pairs of the centroids\n",
    "        centroids = np.array([r[2] for r in results])\n",
    "        D = dist.cdist(centroids, centroids, metric='euclidean')\n",
    "\n",
    "        # loop over the upper triangular of the distance matrix\n",
    "        for i in range(0, D.shape[0]):\n",
    "            for j in range(i+1, D.shape[1]):\n",
    "                # check to see if the distance between any two \n",
    "                # centroid pairs is less than the configured number of pixels\n",
    "                if D[i, j] < MIN_DISTANCE:\n",
    "                    # update our violation set with the indexes of \n",
    "                    # the centroid pairs is less than configured number of pixels\n",
    "                    violate.add(i)\n",
    "                    violate.add(j)\n",
    "\n",
    "    # loop over the results\n",
    "    for (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "        # extract the bounding box and centroid coordinates, then\n",
    "        # initialize the color of the annotation\n",
    "        (startX, startY, endX, endY) = bbox\n",
    "        (cX, cY) = centroid\n",
    "        color = (0, 255, 0)\n",
    "\n",
    "        # if the index pair exists within the violation set, \n",
    "        # then, update the color \n",
    "        if i in violate:\n",
    "            color = (0, 0, 255)\n",
    "\n",
    "        # draw (1) a bounding box around the person and \n",
    "        # (2) centroid coordinaes of the person\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "        cv2.circle(frame, (cX, cY), 5, color, 1)\n",
    "\n",
    "        # draw the total number of social distancing violations on the output frame\n",
    "        text = \"Social Distancing Violations: {}\".format(len(violate))\n",
    "        cv2.putText(frame, text, (10, frame.shape[0] - 25),\n",
    "              cv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\n",
    "  \n",
    "        # Show frame\n",
    "        cv2.imshow(\"Output\", frame)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "        # Write the output to directory    \n",
    "        if writer is None:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            writer = cv2.VideoWriter(output_data, fourcc, 60, \n",
    "                            (frame.shape[1], frame.shape[0]), True)\n",
    "        else:\n",
    "            writer.write(frame)\n",
    "\n",
    "cv2.destroyAllWindows()   \n",
    "writer.release()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc91410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
